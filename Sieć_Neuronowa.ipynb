{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SieÄ‡_Neuronowa.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOYlop4sPasc9Mwl/hpVqFl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HerbertDaca/projekt_IL2/blob/main/Sie%C4%87_Neuronowa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCN3pY_KVP3Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import regex as re\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import regularizers, optimizers\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout, Input, LSTM, GlobalMaxPool1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.initializers import Constant\n",
        "import spacy\n",
        "import wandb\n",
        "wandb.login(key= \"bdd577b3c2ee944f33f7fc061b8def672936904f\")\n",
        "from wandb.keras import WandbCallback\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "academic_bearer = \"AAAAAAAAAAAAAAAAAAAAADIEawEAAAAAxzzD4cQ2g8FGK2%2BkKz2%2FJvTnoMA%3D09uegYs5HrQvrsFkAEl3WwxhspBYFBIH3Vnykec79asqiUsSoA\"\n",
        "client = tweepy.Client(academic_bearer)\n",
        "\n"
      ],
      "metadata": {
        "id": "GOyrckqjbhmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "RETWEETED_REGEX = r\"^RT @([^ ]+):\"\n",
        "USER_REGEX = r\"@([^ ]+)\"\n",
        "URL_REGEX = r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\"\n",
        "\n",
        "def clean_tweet(tweet):\n",
        "    is_retweet = False if re.match(RETWEETED_REGEX, tweet) is None else True\n",
        "    di = {\"is_retweet\": is_retweet,\n",
        "          \"retweeted_user\": re.findall(RETWEETED_REGEX, tweet)[0] if is_retweet else \"\",\n",
        "          \"links\": re.findall(URL_REGEX, tweet)}\n",
        "    tweet = re.sub(RETWEETED_REGEX, \"\", tweet)\n",
        "    di[\"mentioned_users\"] = re.findall(USER_REGEX, tweet)\n",
        "    tweet = re.sub(USER_REGEX, \"\", tweet)\n",
        "    tweet = re.sub(URL_REGEX, \"\", tweet)\n",
        "    tweet = re.sub(\"\\n\", \" \", tweet)\n",
        "    di[\"cleaned_tweet\"] = tweet.strip()\n",
        "    return di\n",
        "\n"
      ],
      "metadata": {
        "id": "0pgmlVHbbhzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tweet_fields = ['created_at', 'public_metrics']\n",
        "query = 'place_country:PL lang:PL'\n",
        "\n",
        "\n",
        "\n",
        "max_result = 500\n",
        "number_of_wanted_tweets = 1000\n",
        "licznik = 0\n",
        "\n",
        "file_name2 = \"many_tweets2.tsv\"\n",
        "file_name = \"many_tweets.tsv\"\n",
        "with open(file_name, 'w+', encoding= \"utf-8\") as filehandle:\n",
        "    with open(file_name2, 'w+', encoding=\"utf-8\") as filehandle2:\n",
        "        for tweet in tweepy.Paginator(client.search_all_tweets, query=query, tweet_fields=tweet_fields, max_results=max_result, sort_order = \"relevancy\").flatten(limit=number_of_wanted_tweets):\n",
        "            tweet_dict = clean_tweet(tweet.text)\n",
        "            tweet_czysty = tweet_dict[\"cleaned_tweet\"]\n",
        "            licznik += 1\n",
        "            filehandle.write(f\"{tweet_czysty}\\t{tweet.id}\\n\")\n",
        "            filehandle2.write(\n",
        "                f\"{6 * tweet.public_metrics['reply_count'] + tweet.public_metrics['like_count'] + 6 * tweet.public_metrics['retweet_count']} ,\"\n",
        "                f\"{tweet.id} ,\"\n",
        "                f\"{tweet.created_at} ,\"\n",
        "                f\"{licznik}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "DNc9GTWJbh3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv(filepath_or_buffer=\"C:/Users/PC/PycharmProjects/TWEEPY/many_tweets2.tsv\", names= (\"points\", \"tweet_id\", \"created_at\", \"nr\"))\n",
        "\n",
        "\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "KE_bJZfobh8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kwantyl = df.points.quantile(0.95)\n",
        "df2=df.assign(labelki=lambda x: x.points > kwantyl)\n",
        "df2['labelki'] = df2['labelki'].astype(int)\n",
        "\n",
        "df2.drop(df.iloc[:, 0:4], inplace=True, axis=1)\n",
        "\n",
        "df2.to_csv(path_or_buf=\"C:/Users/PC/PycharmProjects/TWEEPY/many_tweets3.tsv\", encoding=\"utf-8\", header= False, index_label= False, index= False)"
      ],
      "metadata": {
        "id": "cfe6_dLJcNeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train_labels = pd.read_csv(\"C:/Users/PC/PycharmProjects/TWEEPY/many_tweets3.tsv\")\n",
        "train_data = pd.read_csv(\"C:/Users/PC/PycharmProjects/TWEEPY/many_tweets.tsv\", sep = \"\\t\").dropna()\n",
        "\n",
        "print(train_data.iloc[:, 0])\n",
        "\n",
        "# nlp = en_core_web_lg.load()\n",
        "# Vectorizer = TextVectorization()\n",
        "# x = Vectorizer.adapt(test_data.iloc[:, 0].to_numpy())\n",
        "# vocab = Vectorizer.get_vocabulary()\n",
        "# print(x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mVt-AKF-cUxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Vectorizer = TextVectorization()\n",
        "\n",
        "Vectorizer.adapt(train_data.iloc[:,0].to_numpy())\n",
        "vocab = Vectorizer.get_vocabulary()\n",
        "print(len(vocab))\n",
        "num_tokens = len(vocab)\n",
        "\n",
        "for i, word in enumerate(vocab):\n",
        "    embedding_matrix[i] = nlp(str(word)).vector"
      ],
      "metadata": {
        "id": "EXkNslhQWQrB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}